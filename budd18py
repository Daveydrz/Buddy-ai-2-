import ctypes
import os
import re
import json
import time
import queue
import threading
import tempfile
import concurrent.futures
from pathlib import Path
from pydub import AudioSegment
from pydub.playbook import _play_with_simpleaudio as play
import numpy as np
import pvporcupine
import pyaudio
import requests
import sounddevice as sd
import websockets
import asyncio
import webrtcvad
from langdetect import detect, detect_langs
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from scipy.io.wavfile import write
from kokoro_onnx import Kokoro
import soundfile as sf
from scipy.signal import resample, resample_poly, butter, sosfilt, hilbert
from scipy.ndimage import uniform_filter1d
import random
import io
import wave
import simpleaudio as sa
import traceback
import array
from textblob import TextBlob
from io import BytesIO
import difflib
from resemblyzer import VoiceEncoder

# Enhanced audio processing with better AEC
from webrtc_audio_processing import AudioProcessingModule

encoder = VoiceEncoder()
last_tts_audio = None  # Global buffer to track Buddy's last spoken waveform
last_flavor = None 

# ========== ENHANCED AUDIO PROCESSING CONFIG ==========
WEBRTC_SAMPLE_RATE = 16000
WEBRTC_FRAME_SIZE = 160  # 10ms frames for optimal WebRTC processing
WEBRTC_CHANNELS = 1
MIC_DEVICE_INDEX = 60
MIC_SAMPLE_RATE = 48000
SPEAKER_DEVICE_INDEX = 12

# Enhanced AEC settings
AEC_DELAY_MS = 50  # Typical speaker-to-mic delay
AEC_FILTER_LENGTH = 512  # Longer filter for better echo cancellation
NOISE_GATE_THRESHOLD = -40  # dB threshold for noise gate
AGC_TARGET_LEVEL = -20  # dB target for automatic gain control

# VAD and interruption settings
VAD_AGGRESSIVENESS = 2  # 0-3, higher = more aggressive
MIN_SPEECH_DURATION = 0.3  # seconds
SILENCE_TIMEOUT = 1.5  # seconds
BARGE_IN_DELAY = 0.8  # seconds to wait before allowing interruption

# ========== CONFIG & PATHS ==========
CHIME_PATH = "chime.wav"
known_users_path = "known_users.json"
THEMES_PATH = "themes_memory"
LAST_USER_PATH = "last_user.json"
FASTER_WHISPER_WS = "ws://localhost:9090"
SERPAPI_KEY = os.environ.get("SERPAPI_KEY", "")
SERPAPI_ENDPOINT = "https://serpapi.com/search"
WEATHERAPI_KEY = os.environ.get("WEATHERAPI_KEY", "")
HOME_ASSISTANT_URL = os.environ.get("HOME_ASSISTANT_URL", "http://localhost:8123")
HOME_ASSISTANT_TOKEN = os.environ.get("HOME_ASSISTANT_TOKEN", "")
KOKORO_VOICES = {"pl": "af_heart", "en": "af_heart", "it": "if_sara"}
KOKORO_LANGS = {"pl": "pl", "en": "en-us", "it": "it"}
DEFAULT_LANG = "en"
FAST_MODE = True
DEBUG = True
DEBUG_MODE = False
BUDDY_BELIEFS_PATH = "buddy_beliefs.json"
LONG_TERM_MEMORY_PATH = "buddy_long_term_memory.json"
PERSONALITY_TRAITS_PATH = "buddy_personality_traits.json"
DYNAMIC_KNOWLEDGE_PATH = "buddy_dynamic_knowledge.json"
BYPASS_AEC = False

# ========== ENHANCED AUDIO PROCESSING MODULE ==========
class EnhancedAudioProcessor:
    def __init__(self):
        self.aec_module = AudioProcessingModule()
        self.setup_aec()
        
        # Circular buffers for reference audio (speaker output)
        self.ref_buffer_size = int(WEBRTC_SAMPLE_RATE * 2)  # 2 seconds buffer
        self.ref_buffer = np.zeros(self.ref_buffer_size, dtype=np.float32)
        self.ref_write_pos = 0
        self.ref_lock = threading.RLock()
        
        # Adaptive filters
        self.adaptive_filter = np.zeros(AEC_FILTER_LENGTH, dtype=np.float32)
        self.filter_update_rate = 0.01
        
        # Noise estimation
        self.noise_floor = np.ones(513) * 1e-6  # For 1024 FFT
        self.noise_alpha = 0.95
        
        # AGC state
        self.agc_gain = 1.0
        self.agc_alpha = 0.999
        
        # VAD enhancement
        self.vad_history = []
        self.speech_prob_threshold = 0.6
        
    def setup_aec(self):
        """Configure WebRTC audio processing with optimal settings"""
        self.aec_module.set_stream_format(WEBRTC_SAMPLE_RATE, WEBRTC_CHANNELS)
        
        # Echo cancellation settings
        self.aec_module.set_aec_level(2)  # High quality AEC
        self.aec_module.set_delay_agnostic_aec(True)  # Handle variable delays
        
        # Noise suppression
        self.aec_module.set_ns_level(2)  # High noise suppression
        
        # Automatic gain control
        self.aec_module.set_agc_level(2)  # Adaptive gain
        self.aec_module.set_agc_target_level(AGC_TARGET_LEVEL)
        
        # VAD configuration
        self.aec_module.set_vad_level(2)  # Medium VAD sensitivity
        
        # High-pass filter to remove low-frequency noise
        self.aec_module.set_highpass_filter(True)
        
    def add_reference_audio(self, audio_data):
        """Add speaker output to reference buffer for echo cancellation"""
        if isinstance(audio_data, bytes):
            audio_np = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
        else:
            audio_np = audio_data.astype(np.float32)
            if audio_np.dtype == np.int16:
                audio_np = audio_np / 32768.0
        
        with self.ref_lock:
            # Add to circular buffer
            samples_to_add = len(audio_np)
            if samples_to_add > self.ref_buffer_size:
                audio_np = audio_np[-self.ref_buffer_size:]
                samples_to_add = self.ref_buffer_size
            
            # Handle wrap-around
            end_pos = self.ref_write_pos + samples_to_add
            if end_pos <= self.ref_buffer_size:
                self.ref_buffer[self.ref_write_pos:end_pos] = audio_np
            else:
                # Split write
                first_part = self.ref_buffer_size - self.ref_write_pos
                self.ref_buffer[self.ref_write_pos:] = audio_np[:first_part]
                self.ref_buffer[:end_pos - self.ref_buffer_size] = audio_np[first_part:]
            
            self.ref_write_pos = end_pos % self.ref_buffer_size
    
    def get_reference_audio(self, num_samples):
        """Get recent reference audio for echo cancellation"""
        with self.ref_lock:
            if num_samples > self.ref_buffer_size:
                num_samples = self.ref_buffer_size
            
            start_pos = (self.ref_write_pos - num_samples) % self.ref_buffer_size
            
            if start_pos + num_samples <= self.ref_buffer_size:
                return self.ref_buffer[start_pos:start_pos + num_samples].copy()
            else:
                # Handle wrap-around
                first_part = self.ref_buffer_size - start_pos
                result = np.zeros(num_samples, dtype=np.float32)
                result[:first_part] = self.ref_buffer[start_pos:]
                result[first_part:] = self.ref_buffer[:num_samples - first_part]
                return result
    
    def spectral_subtraction(self, audio_fft, noise_floor):
        """Apply spectral subtraction for noise reduction"""
        magnitude = np.abs(audio_fft)
        phase = np.angle(audio_fft)
        
        # Calculate noise reduction factor
        snr = magnitude / (noise_floor + 1e-10)
        reduction_factor = np.maximum(0.1, 1.0 - (2.0 / (snr + 1e-10)))
        
        # Apply reduction
        cleaned_magnitude = magnitude * reduction_factor
        return cleaned_magnitude * np.exp(1j * phase)
    
    def adaptive_noise_estimation(self, audio_fft, is_speech):
        """Update noise floor estimation"""
        magnitude = np.abs(audio_fft)
        
        if not is_speech:
            # Update noise floor during non-speech segments
            self.noise_floor = (self.noise_alpha * self.noise_floor + 
                              (1 - self.noise_alpha) * magnitude)
    
    def apply_agc(self, audio):
        """Apply automatic gain control"""
        # Calculate RMS level
        rms = np.sqrt(np.mean(audio ** 2))
        
        if rms > 1e-6:  # Avoid division by zero
            target_rms = 10 ** (AGC_TARGET_LEVEL / 20.0)
            desired_gain = target_rms / rms
            
            # Smooth gain changes
            self.agc_gain = (self.agc_alpha * self.agc_gain + 
                           (1 - self.agc_alpha) * desired_gain)
            
            # Limit gain to prevent distortion
            self.agc_gain = np.clip(self.agc_gain, 0.1, 10.0)
            
            return audio * self.agc_gain
        
        return audio
    
    def enhanced_vad(self, audio_chunk):
        """Enhanced VAD with temporal smoothing"""
        # Convert to int16 for WebRTC VAD
        if audio_chunk.dtype != np.int16:
            audio_int16 = (audio_chunk * 32767).astype(np.int16)
        else:
            audio_int16 = audio_chunk
        
        # WebRTC VAD
        vad = webrtcvad.Vad(VAD_AGGRESSIVENESS)
        is_speech = vad.is_speech(audio_int16.tobytes(), WEBRTC_SAMPLE_RATE)
        
        # Add to history for temporal smoothing
        self.vad_history.append(1.0 if is_speech else 0.0)
        if len(self.vad_history) > 10:  # Keep last 10 frames (100ms)
            self.vad_history.pop(0)
        
        # Smooth decision
        speech_probability = np.mean(self.vad_history)
        return speech_probability > self.speech_prob_threshold
    
    def process_microphone_audio(self, mic_audio):
        """Main processing pipeline for microphone audio"""
        try:
            # Convert input to float32
            if isinstance(mic_audio, bytes):
                audio = np.frombuffer(mic_audio, dtype=np.int16).astype(np.float32) / 32768.0
            else:
                audio = mic_audio.astype(np.float32)
                if audio.dtype == np.int16:
                    audio = audio / 32768.0
            
            # Ensure correct length for WebRTC processing
            if len(audio) != WEBRTC_FRAME_SIZE:
                if len(audio) > WEBRTC_FRAME_SIZE:
                    audio = audio[:WEBRTC_FRAME_SIZE]
                else:
                    audio = np.pad(audio, (0, WEBRTC_FRAME_SIZE - len(audio)), 'constant')
            
            # Get reference audio for echo cancellation
            ref_audio = self.get_reference_audio(WEBRTC_FRAME_SIZE)
            
            # Convert to int16 for WebRTC processing
            mic_int16 = (audio * 32767).astype(np.int16)
            ref_int16 = (ref_audio * 32767).astype(np.int16)
            
            # Apply reference audio to AEC
            if np.any(ref_int16):
                self.aec_module.process_reverse_stream(ref_int16.tobytes())
            
            # Process microphone stream
            processed_bytes = self.aec_module.process_stream(mic_int16.tobytes())
            processed_audio = np.frombuffer(processed_bytes, dtype=np.int16).astype(np.float32) / 32768.0
            
            # Enhanced VAD
            is_speech = self.enhanced_vad(processed_audio)
            
            # Spectral processing for additional noise reduction
            if len(processed_audio) >= 512:  # Minimum for FFT
                # Apply FFT
                fft_audio = np.fft.rfft(processed_audio, n=1024)
                
                # Update noise estimation
                self.adaptive_noise_estimation(fft_audio, is_speech)
                
                # Apply spectral subtraction
                if not is_speech:
                    fft_audio = self.spectral_subtraction(fft_audio, self.noise_floor)
                
                # Convert back to time domain
                processed_audio = np.fft.irfft(fft_audio)[:len(processed_audio)]
            
            # Apply AGC
            processed_audio = self.apply_agc(processed_audio)
            
            # Convert back to int16
            output = (processed_audio * 32767).astype(np.int16)
            
            return output, is_speech
            
        except Exception as e:
            print(f"[AEC] Error in process_microphone_audio: {e}")
            # Return original audio on error
            if isinstance(mic_audio, bytes):
                return np.frombuffer(mic_audio, dtype=np.int16), False
            return mic_audio.astype(np.int16), False

# ========== GLOBAL STATE ==========
audio_processor = EnhancedAudioProcessor()
aec_module = AudioProcessingModule()
aec_module.set_stream_format(WEBRTC_SAMPLE_RATE, WEBRTC_CHANNELS)
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
executor = concurrent.futures.ThreadPoolExecutor(max_workers=3)
kokoro = Kokoro("kokoro-v1.0.onnx", "voices-v1.0.bin")
os.makedirs(THEMES_PATH, exist_ok=True)

# Legacy ref buffer for backward compatibility
ref_audio_buffer = np.zeros(WEBRTC_SAMPLE_RATE * 2, dtype=np.int16)  # 2 seconds
ref_audio_lock = threading.Lock()

# Threading and queue management
playback_lock = threading.Lock()
tts_lock = threading.Lock()
tts_queue = queue.Queue()
playback_queue = queue.Queue()
current_playback = None
playback_stop_flag = threading.Event()
buddy_talking = threading.Event()
vad_triggered = threading.Event()
full_duplex_interrupt_flag = threading.Event()
full_duplex_vad_result = queue.Queue()

# Memory and state
LAST_FEW_BUDDY = []
RECENT_WHISPER = []
known_users = {}
active_speakers = {}
active_speaker_lock = threading.Lock()
session_emotion_mode = {}  # user: mood for mood injection
tts_start_time = 0

# Load existing data
if os.path.exists(known_users_path):
    with open(known_users_path, "r", encoding="utf-8") as f:
        known_users = json.load(f)

if DEBUG:
    device = "cuda" if 'cuda' in os.environ.get('CUDA_VISIBLE_DEVICES', '') or hasattr(np, "cuda") else "cpu"
    print(f"[Buddy] Running on device: {device}")
    print("Embedding model loaded", flush=True)
    print("Kokoro loaded", flush=True)
    print("Enhanced audio processor initialized", flush=True)

# ========== AUDIO PROCESSING HELPERS ==========
def downsample(audio, orig_sr, target_sr):
    """Improved downsampling with anti-aliasing"""
    if audio.ndim > 1:
        audio = audio[:, 0]
    
    if audio.dtype == np.int16:
        audio = audio.astype(np.float32) / 32768.0
    
    if orig_sr == target_sr:
        return (audio * 32767).astype(np.int16)
    
    # Apply anti-aliasing filter before downsampling
    nyquist_target = target_sr / 2
    nyquist_orig = orig_sr / 2
    
    if nyquist_target < nyquist_orig:
        # Design lowpass filter
        sos = butter(5, nyquist_target / nyquist_orig, btype='low', output='sos')
        audio = sosfilt(sos, audio)
    
    # Resample
    gcd = np.gcd(orig_sr, target_sr)
    up = target_sr // gcd
    down = orig_sr // gcd
    audio_resampled = resample_poly(audio, up, down)
    
    # Clip and convert
    audio_resampled = np.clip(audio_resampled, -1.0, 1.0)
    return (audio_resampled * 32767).astype(np.int16)

# Legacy AEC function for backward compatibility
def apply_aec(mic_audio, bypass_aec=False):
    try:
        if bypass_aec:
            print("[AEC] Bypassed")
            return mic_audio[:WEBRTC_FRAME_SIZE]

        # === Safely copy reference buffer ===
        with ref_audio_lock:
            ref_copy = ref_audio_buffer.copy()

        if len(ref_copy) < WEBRTC_FRAME_SIZE:
            print(f"[AEC] WARNING: Ref buffer too short ({len(ref_copy)} samples). Padding with zeros.")
            ref_copy = np.pad(ref_copy, (0, WEBRTC_FRAME_SIZE - len(ref_copy)), mode='constant')

        print("[AEC] Ref input range:", ref_copy.min(), ref_copy.max())

        # === Convert mic input to int16 numpy array ===
        if isinstance(mic_audio, bytes):
            mic_np = np.frombuffer(mic_audio, dtype=np.int16)
        else:
            mic_np = np.asarray(mic_audio, dtype=np.int16)

        if len(mic_np) < WEBRTC_FRAME_SIZE:
            print(f"[AEC] WARNING: Mic input too short ({len(mic_np)} samples). Padding.")
            mic_np = np.pad(mic_np, (0, WEBRTC_FRAME_SIZE - len(mic_np)), mode='constant')

        mic_bytes = mic_np[:WEBRTC_FRAME_SIZE].astype(np.int16).tobytes()
        ref_bytes = ref_copy[:WEBRTC_FRAME_SIZE].astype(np.int16).tobytes()

        # === Run AEC ===
        aec_module.process_reverse_stream(ref_bytes)
        output_bytes = aec_module.process_stream(mic_bytes)

        mic_np_cleaned = np.frombuffer(output_bytes, dtype=np.int16)

        # ✅ Clamp output tightly to suppress ghost echo
        mic_np_cleaned = np.clip(mic_np_cleaned, -300, 300)

        print("[AEC] Output cleaned range:", mic_np_cleaned.min(), mic_np_cleaned.max())
        return mic_np_cleaned

    except Exception as e:
        print(f"[AEC] Error in apply_aec: {e}")
        return mic_audio[:WEBRTC_FRAME_SIZE]

from numpy.linalg import norm

def is_echo_of_last_tts(mic_audio, last_tts_audio, threshold=0.9):
    if mic_audio.shape != last_tts_audio.shape:
        min_len = min(len(mic_audio), len(last_tts_audio))
        mic_audio = mic_audio[:min_len]
        last_tts_audio = last_tts_audio[:min_len]

    correlation = np.dot(mic_audio, last_tts_audio) / (norm(mic_audio) * norm(last_tts_audio) + 1e-6)
    return correlation > threshold

# ========== ENHANCED BACKGROUND VAD LISTENER ==========
def enhanced_background_vad_listener():
    """Enhanced full-duplex VAD with better interruption handling"""
    blocksize = int(MIC_SAMPLE_RATE * 0.02)  # 20ms blocks
    min_bargein_delay = BARGE_IN_DELAY
    speech_frames = 0
    min_speech_frames = int(MIN_SPEECH_DURATION / 0.02)  # Convert to frame count
    
    print("[Buddy][FULL-DUPLEX] Enhanced VAD listener started")
    
    try:
        with sd.InputStream(device=MIC_DEVICE_INDEX, samplerate=MIC_SAMPLE_RATE, 
                          channels=1, dtype='int16', blocksize=blocksize) as stream:
            
            while buddy_talking.is_set():
                frame, _ = stream.read(blocksize)
                mic_audio = frame.flatten()
                
                # Downsample to WebRTC sample rate
                mic_16k = downsample(mic_audio, MIC_SAMPLE_RATE, WEBRTC_SAMPLE_RATE)
                
                # Process audio through enhanced pipeline
                processed_audio, is_speech = audio_processor.process_microphone_audio(mic_16k)
                
                # Avoid premature interruption
                if time.time() - tts_start_time < min_bargein_delay:
                    speech_frames = 0
                    continue
                
                # Count consecutive speech frames
                if is_speech:
                    speech_frames += 1
                else:
                    speech_frames = max(0, speech_frames - 2)  # Decay faster
                
                # Trigger interruption if sufficient speech detected
                if speech_frames >= min_speech_frames:
                    print("[Buddy][FULL-DUPLEX] User interruption detected! Stopping playback.")
                    full_duplex_interrupt_flag.set()
                    full_duplex_vad_result.put(processed_audio)
                    stop_playback()
                    break
                    
    except Exception as e:
        print(f"[Buddy][FULL-DUPLEX VAD Error]: {e}")

# Legacy background VAD for backward compatibility
def background_vad_listener():
    vad = webrtcvad.Vad(3)
    blocksize = int(MIC_SAMPLE_RATE * 0.02)
    min_bargein_delay = 2.8  # ⏱ wait this long before enabling barge-in

    try:
        with sd.InputStream(device=MIC_DEVICE_INDEX, samplerate=MIC_SAMPLE_RATE, channels=1,
                            dtype='int16', blocksize=blocksize) as stream:
            print("[Buddy][VAD] Stream started for barge-in monitoring.")

            while buddy_talking.is_set():
                try:
                    frame, _ = stream.read(blocksize)
                except Exception as read_err:
                    print(f"[VAD ERROR] Failed to read from stream: {read_err}")
                    break

                mic_audio = frame.flatten().tobytes()

                mic_np = np.frombuffer(mic_audio, dtype=np.int16)
                print("[VAD] Raw mic input range:", mic_np.min(), mic_np.max())

                aec_fixed = apply_aec(mic_audio)
                audio_16k = downsample(np.frombuffer(aec_fixed, dtype=np.int16), MIC_SAMPLE_RATE, 16000)

                # ✅ Exit if Buddy stopped talking while in loop
                if not buddy_talking.is_set():
                    print("[Buddy] VAD: Buddy finished talking, exiting VAD thread.")
                    break

                # ✅ Avoid premature barge-in too early into Buddy's speech
                if time.time() - tts_start_time < min_bargein_delay:
                    continue

                if vad.is_speech(audio_16k.tobytes(), 16000):
                    print("[Buddy][FULL-DUPLEX] User started speaking during TTS! Interrupting.")
                    full_duplex_interrupt_flag.set()
                    full_duplex_vad_result.put(audio_16k)
                    stop_playback()
                    break

    except Exception as e:
        print(f"[Buddy][FULL-DUPLEX VAD Stream Error]: {e}")

def start_background_vad_thread():
    """Start the enhanced background VAD thread"""
    full_duplex_interrupt_flag.clear()
    thread = threading.Thread(target=enhanced_background_vad_listener, daemon=True)
    thread.start()
    return thread

# ========== MULTI-SPEAKER DETECTION ==========
def detect_active_speaker(audio_chunk):
    embedding = generate_embedding_from_audio(audio_chunk)
    best_name, best_score = match_known_user(embedding)
    if best_name and best_score > 0.8:
        with active_speaker_lock:
            active_speakers[threading.get_ident()] = best_name
    return best_name, best_score

def generate_embedding_from_audio(audio_np):
    """
    Generate a speaker embedding from a numpy waveform (16kHz, mono, int16 or float32).
    """
    if audio_np.dtype != np.float32:
        audio_np = audio_np.astype(np.float32) / 32768.0  # Normalize from int16 to float32
    
    if audio_np.ndim > 1:
        audio_np = audio_np[:, 0]  # ensure mono

    return encoder.embed_utterance(audio_np)

def assign_turn_per_speaker(audio_chunk):
    name, score = detect_active_speaker(audio_chunk)
    if name:
        print(f"[Buddy][Multi-Speaker] Speaker switched to: {name} (score={score:.2f})")
        return name
    return None

# ========== MEMORY HELPERS ==========
def get_user_memory_path(name):
    return f"user_memory_{name}.json"

def load_user_memory(name):
    path = get_user_memory_path(name)
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return {}

def save_user_memory(name, memory):
    path = get_user_memory_path(name)
    with open(path, "w", encoding="utf-8") as f:
        json.dump(memory, f, indent=2, ensure_ascii=False)

def update_user_memory(name, utterance):
    memory = load_user_memory(name)
    text = utterance.lower()
    if re.search(r"\bi('?m| am| feel) sad\b", text):
        memory["mood"] = "sad"
    elif re.search(r"\bi('?m| am| feel) happy\b", text):
        memory["mood"] = "happy"
    elif re.search(r"\bi('?m| am| feel) (angry|mad|upset)\b", text):
        memory["mood"] = "angry"
    if re.search(r"\bi (love|like|enjoy|prefer) (marvel movies|marvel|comics)\b", text):
        hobbies = memory.get("hobbies", [])
        if "marvel movies" not in hobbies:
            hobbies.append("marvel movies")
        memory["hobbies"] = hobbies
    if "issue at work" in text or "problems at work" in text or "problem at work" in text:
        memory["work_issue"] = "open"
    if ("issue" in memory and "solved" in text) or ("work_issue" in memory and ("solved" in text or "fixed" in text)):
        memory["work_issue"] = "resolved"
    save_user_memory(name, memory)

def build_user_facts(name):
    memory = load_user_memory(name)
    facts = []
    if "mood" in memory:
        facts.append(f"The user was previously {memory['mood']}.")
    if "hobbies" in memory:
        facts.append(f"The user likes: {', '.join(memory['hobbies'])}.")
    if memory.get("work_issue") == "open":
        facts.append(f"The user had unresolved issues at work.")
    return facts

# ========== HISTORY & THEMES ==========
def load_user_history(name):
    path = f"history_{name}.json"
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

def save_user_history(name, history):
    path = f"history_{name}.json"
    with open(path, "w", encoding="utf-8") as f:
        json.dump(history[-20:], f, ensure_ascii=False, indent=2)

def extract_topic_from_text(text):
    words = re.findall(r'\b\w+\b', text.lower())
    freq = {}
    for w in words:
        if len(w) < 4:
            continue
        freq[w] = freq.get(w, 0) + 1
    if freq:
        return max(freq, key=freq.get)
    return None

def update_thematic_memory(user, utterance):
    topic = extract_topic_from_text(utterance)
    if not topic:
        return

    theme_path = os.path.join(THEMES_PATH, f"{user}_themes.json")
    themes = {}

    if os.path.exists(theme_path):
        try:
            with open(theme_path, "r", encoding="utf-8") as f:
                themes = json.load(f)
        except json.JSONDecodeError:
            print(f"[Buddy][Memory] Corrupted theme file for {user}. Reinitializing.")
            themes = {}

    themes[topic] = themes.get(topic, 0) + 1

    with open(theme_path, "w", encoding="utf-8") as f:
        json.dump(themes, f, ensure_ascii=False, indent=2)

def get_frequent_topics(user, top_n=3):
    theme_path = os.path.join(THEMES_PATH, f"{user}_themes.json")
    if not os.path.exists(theme_path):
        return []
    with open(theme_path, "r", encoding="utf-8") as f:
        themes = json.load(f)
    sorted_themes = sorted(themes.items(), key=lambda x: x[1], reverse=True)
    return [topic for topic, _ in sorted_themes[:top_n]]

# ========== EMBEDDING ==========
def generate_embedding(text):
    return embedding_model.encode([text])[0]

def match_known_user(new_embedding, threshold=0.75):
    best_name, best_score = None, 0
    for name, emb in known_users.items():
        sim = cosine_similarity([new_embedding], [emb])[0][0]
        if sim > best_score:
            best_name, best_score = name, sim
    return (best_name, best_score) if best_score >= threshold else (None, best_score)

# ========== MEMORY TIMELINE & SUMMARIZATION ==========
def get_memory_timeline(name, since_days=1):
    path = f"history_{name}.json"
    if not os.path.exists(path):
        return []
    with open(path, "r", encoding="utf-8") as f:
        history = json.load(f)
    cutoff = time.time() - since_days * 86400
    filtered = [x for x in history if x.get("timestamp", 0) > cutoff]
    return filtered

def get_last_conversation(name):
    path = f"history_{name}.json"
    if not os.path.exists(path):
        return None
    with open(path, "r", encoding="utf-8") as f:
        history = json.load(f)
    if not history:
        return None
    return history[-1]

def summarize_history(name, theme=None):
    path = f"history_{name}.json"
    if not os.path.exists(path):
        return "No history found."
    with open(path, "r", encoding="utf-8") as f:
        history = json.load(f)
    utterances = [h["user"] for h in history]
    if theme:
        utterances = [u for u in utterances if theme in u.lower()]
    if utterances:
        summary = f"User mostly talked about: {', '.join(list(set(utterances))[:3])}."
    else:
        summary = "No data to summarize."
    return summary

def summary_bubble_gui(name):
    topics = get_frequent_topics(name, top_n=5)
    facts = build_user_facts(name)
    return {"topics": topics, "facts": facts}

# ========== PROMPT INJECTION PROTECTION ==========
def sanitize_user_prompt(text):
    forbidden = ["ignore previous", "act as", "system:"]
    for f in forbidden:
        if f in text.lower():
            text = text.replace(f, "")
    text = re.sub(r"`{3,}.*?`{3,}", "", text, flags=re.DOTALL)
    return text

# ========== WHISPER STT WITH CONFIDENCE ==========
def stt_stream(audio):
    async def ws_stt(audio):
        try:
            if audio.dtype != np.int16:
                if np.issubdtype(audio.dtype, np.floating):
                    audio = (audio * 32767).clip(-32768, 32767).astype(np.int16)
                else:
                    audio = audio.astype(np.int16)
            print(f"[DEBUG] Sending audio with shape {audio.shape}, dtype: {audio.dtype}, max: {audio.max()}, min: {audio.min()}")
            async with websockets.connect(FASTER_WHISPER_WS, ping_interval=None) as ws:
                await ws.send(audio.tobytes())
                await ws.send("end")
                try:
                    message = await asyncio.wait_for(ws.recv(), timeout=18)
                except asyncio.TimeoutError:
                    print("[Buddy] Whisper timeout. Brak odpowiedzi przez 18s.")
                    return ""
                try:
                    data = json.loads(message)
                    text = data.get("text", "")
                    avg_logprob = data.get("avg_logprob", None)
                    no_speech_prob = data.get("no_speech_prob", None)
                    print(f"[Buddy][Whisper JSON] text={text!r}, avg_logprob={avg_logprob}, no_speech_prob={no_speech_prob}")
                    if whisper_confidence_low(text, avg_logprob, no_speech_prob):
                        print("[Buddy][Whisper] Rejected low-confidence STT result.")
                        return ""
                    return text
                except Exception:
                    text = message.decode("utf-8") if isinstance(message, bytes) else message
                    print(f"\n[Buddy] === Whisper rozpoznał: \"{text}\" ===")
                    return text
        except Exception as e:
            print(f"[Buddy] Błąd połączenia z Whisper: {e}")
            return ""
    return asyncio.run(ws_stt(audio))

def whisper_confidence_low(text, avg_logprob, no_speech_prob):
    if avg_logprob is not None and avg_logprob < -1.2:
        return True
    if no_speech_prob is not None and no_speech_prob > 0.5:
        return True
    if not text or len(text.strip()) < 2:
        return True
    return False

# ========== ENHANCED VAD AND LISTEN ==========
def enhanced_vad_and_listen():
    """Enhanced VAD with better noise handling and interruption support"""
    blocksize = int(MIC_SAMPLE_RATE * 0.02)  # 20ms blocks
    min_speech_frames = int(MIN_SPEECH_DURATION / 0.02)
    silence_frames = int(SILENCE_TIMEOUT / 0.02)
    max_recording_time = 10.0  # Maximum recording time in seconds

    with sd.InputStream(device=MIC_DEVICE_INDEX, samplerate=MIC_SAMPLE_RATE, 
                       channels=1, blocksize=blocksize, dtype='int16') as stream:
        
        print("\n[Buddy] === Enhanced listening mode active ===")
        audio_buffer = []
        speech_frames = 0
        silence_count = 0
        recording = False
        start_time = None

        while True:
            frame, _ = stream.read(blocksize)
            mic_audio = frame.flatten()
            
            # Downsample for processing
            mic_16k = downsample(mic_audio, MIC_SAMPLE_RATE, WEBRTC_SAMPLE_RATE)
            
            # Process through enhanced pipeline
            processed_audio, is_speech = audio_processor.process_microphone_audio(mic_16k)
            
            if is_speech:
                speech_frames += 1
                silence_count = 0
                
                if not recording and speech_frames >= min_speech_frames:
                    print("[Buddy] Speech detected. Starting recording...")
                    recording = True
                    start_time = time.time()
                    audio_buffer = []
                    
                if recording:
                    audio_buffer.append(processed_audio)
                    
            else:
                if recording:
                    silence_count += 1
                    audio_buffer.append(processed_audio)
                    
                    # Check for end of speech
                    if silence_count >= silence_frames:
                        print("[Buddy] End of speech detected. Processing...")
                        break
                        
                    # Check for timeout
                    if time.time() - start_time > max_recording_time:
                        print("[Buddy] Recording timeout. Processing...")
                        break
                else:
                    speech_frames = max(0, speech_frames - 1)

        if audio_buffer:
            # Concatenate all audio chunks
            full_audio = np.concatenate(audio_buffer, axis=0)
            print(f"[Buddy] Recorded {len(full_audio)} samples for processing")
            return full_audio
        else:
            return np.array([], dtype=np.int16)

# Legacy VAD function
def vad_and_listen():
    vad = webrtcvad.Vad(3)  # Lower sensitivity to avoid background noise
    blocksize = int(MIC_SAMPLE_RATE * 0.02)  # 960 samples for 20ms at 48kHz
    min_speech_frames = 10
    silence_thresh = 1.0

    with sd.InputStream(device=MIC_DEVICE_INDEX, samplerate=MIC_SAMPLE_RATE, channels=1, blocksize=blocksize, dtype='int16') as stream:
        print("\n[Buddy] === SŁUCHAM, mów do mnie... ===")
        frame_buffer = []
        speech_detected = 0

        while True:
            frame, _ = stream.read(blocksize)
            mic = np.frombuffer(frame.tobytes(), dtype=np.int16)
            mic_16k = downsample(mic, MIC_SAMPLE_RATE, 16000)

            for i in range(0, len(mic_16k), 160):
                chunk = mic_16k[i:i+160]
                if len(chunk) < 160:
                    continue

                with ref_audio_lock:
                    aec_module.process_reverse_stream(ref_audio_buffer.tobytes())

                processed = aec_module.process_stream(chunk.tobytes())
                chunk_out = np.frombuffer(processed, dtype=np.int16)

                # 🔇 Filter out quiet chunks (likely TV / Buddy's voice)
                volume = np.abs(chunk_out).mean()
                if volume < 700:
                    continue

                if vad.is_speech(chunk_out.tobytes(), 16000):
                    frame_buffer.append(chunk_out)
                    speech_detected += 1

                    if speech_detected >= min_speech_frames:
                        print("[Buddy] VAD: Wykryto mowę. Nagrywam...")
                        audio = frame_buffer.copy()
                        last_speech = time.time()
                        start_time = time.time()
                        frame_buffer.clear()

                        while time.time() - last_speech < silence_thresh and (time.time() - start_time) < 8:
                            frame, _ = stream.read(blocksize)
                            mic = np.frombuffer(frame.tobytes(), dtype=np.int16)
                            mic_16k = downsample(mic, MIC_SAMPLE_RATE, 16000)

                            for j in range(0, len(mic_16k), 160):
                                chunk2 = mic_16k[j:j+160]
                                if len(chunk2) < 160:
                                    continue

                                with ref_audio_lock:
                                    aec_module.process_reverse_stream(ref_audio_buffer.tobytes())

                                processed2 = aec_module.process_stream(chunk2.tobytes())
                                chunk_out2 = np.frombuffer(processed2, dtype=np.int16)

                                # 🔇 Again filter quiet background
                                if np.abs(chunk_out2).mean() < 300:
                                    continue

                                audio.append(chunk_out2)

                                if vad.is_speech(chunk_out2.tobytes(), 16000):
                                    last_speech = time.time()

                        print("[Buddy] Koniec nagrania. Wysyłam do Whisper...")
                        audio_np = np.concatenate(audio, axis=0).astype(np.int16)
                        return audio_np
                else:
                    if len(frame_buffer) > 0:
                        frame_buffer.clear()
                    speech_detected = 0

def enhanced_fast_listen_and_transcribe(history=None):
    """Enhanced listening with better audio processing"""
    wait_after_buddy_speaks(delay=0.3)
    
    # Check for interruption during TTS
    if not full_duplex_vad_result.empty():
        print("[Buddy] Using interrupted audio from full-duplex VAD")
        interrupted_audio = full_duplex_vad_result.get()
        # Continue recording for a bit more
        additional_audio = enhanced_vad_and_listen()
        if len(additional_audio) > 0:
            audio = np.concatenate([interrupted_audio, additional_audio])
        else:
            audio = interrupted_audio
    else:
        audio = enhanced_vad_and_listen()
    
    if len(audio) == 0:
        return "..."
    
    # Save for debugging
    try:
        write("temp_input.wav", WEBRTC_SAMPLE_RATE, audio)
        print(f"[DEBUG] Saved enhanced audio: shape={audio.shape}, dtype={audio.dtype}")
    except Exception as e:
        print(f"[Buddy] Error saving audio: {e}")
    
    # Send to Whisper
    text = stt_stream(audio).strip()
    cleaned = re.sub(r'[^\w\s]', '', text.lower())

    # ✅ Skip if nothing detected
    if not text or len(cleaned) < 2:
        print("[DEBUG] Empty or too short. Exiting interaction.")
        return "..."

    # ✅ Skip if it's a direct echo of Buddy's recent output
    if is_echo(cleaned):
        print(f"[Buddy] Skipping echo: {cleaned}")
        return "..."

    # ✅ Skip if it's a meaningless confirmation (loop trigger)
    if is_useless_confirmation(cleaned):
        print(f"[Buddy] Skipping useless confirmation: {text}")
        return "..."

    # ✅ Noise/gibberish filter
    if is_noise_or_gibberish(text):
        print(f"[Buddy] Skipping gibberish: {text}")
        return "..."

    # ✅ Prevent exact or near-duplicate user repeat
    if history and "user" in history[-1]:
        last_user = history[-1]["user"].strip().lower()
        ratio = difflib.SequenceMatcher(None, last_user, cleaned).ratio()
        if ratio > 0.95:
            print(f"[Buddy] Skipping redundant user input (sim={ratio:.2f})")
            return "..."

    # ✅ Prevent reprocessing Buddy's most recent reply (text match)
    if history and "buddy" in history[-1]:
        last_reply = history[-1]["buddy"].strip().lower()
        if cleaned in last_reply:
            print(f"[Buddy] Detected echo of last Buddy message: {cleaned}")
            return "..."

    # ✅ Track for future echo prevention
    if cleaned:
        RECENT_WHISPER.append(cleaned)
        if len(RECENT_WHISPER) > 5:
            RECENT_WHISPER.pop(0)

    return text

def is_noise_or_gibberish(text):
    if not text or len(text.strip()) == 0:
        return True

    cleaned = re.sub(r'[^\w\s]', '', text.strip().lower())
    
    # Reject short or meaningless utterances
    if len(cleaned.split()) <= 1 and len(cleaned) < 6:
        return True

    # Reject if it matches recent Buddy lines (echo filter)
    if cleaned in [x.lower().strip() for x in LAST_FEW_BUDDY[-3:]]:
        print("[Buddy] Ignored echo of recent Buddy response.")
        return True

    # Reject background-like or filler speech
    nonsense_keywords = ["hmm", "uh", "um", "err", "eh", "oh", "ah", "yeah", "mmm", "what", "huh"]
    return any(word in cleaned for word in nonsense_keywords)

def is_echo(text):
    if not text or len(text.strip()) < 3:
        return False

    cleaned = re.sub(r'[^\w\s]', '', text.strip().lower())
    if not cleaned or len(cleaned.split()) < 2:
        return False

    # Block known throwaway confirmations
    if cleaned in {"okay", "sure", "cool", "nice", "awesome", "same here", "me too"}:
        print(f"[Buddy] Skipping known confirmation echo: {cleaned}")
        return True

    for prev in LAST_FEW_BUDDY[-3:]:
        prev_clean = re.sub(r'[^\w\s]', '', prev.strip().lower())

        # Fuzzy match
        ratio = difflib.SequenceMatcher(None, cleaned, prev_clean).ratio()
        word_diff = abs(len(cleaned.split()) - len(prev_clean.split()))
        if ratio > 0.87 and word_diff <= 4:
            print(f"[Buddy] Skipping echo:\n→ new:  {cleaned}\n→ prev: {prev_clean}\n→ sim={ratio:.2f}, word_diff={word_diff}")
            return True

        # Partial containment
        if cleaned in prev_clean:
            print(f"[Buddy] Skipping partial echo match: {cleaned} in {prev_clean}")
            return True

    return False

def is_useless_confirmation(text):
    confirmations = [
        "glad to hear it", "same here", "me too", "good to know",
        "that's nice", "cool", "nice", "awesome", "sure", "okay", "alright",
        "I'm glad to hear it", "I am glad to hear it", "same for me"
    ]
    text = text.strip().lower()
    return any(text == c or text.startswith(c) for c in confirmations)

def fast_listen_and_transcribe(history=None):
    """Legacy function - choose between enhanced or legacy version"""
    # Use enhanced version by default
    return enhanced_fast_listen_and_transcribe(history)

# ========== TTS & ENHANCED PLAYBACK ==========
def generate_and_play_kokoro(text, lang):
    """Generate TTS audio using Kokoro and queue for playback"""
    global tts_start_time, last_tts_audio
    tts_start_time = time.time()
    
    try:
        voice = KOKORO_VOICES.get(lang, KOKORO_VOICES["en"])
        kokoro_lang = KOKORO_LANGS.get(lang, KOKORO_LANGS["en"])
        
        print(f"[Buddy][TTS] Generating: '{text}' with voice={voice}, lang={kokoro_lang}")
        
        # Generate audio with Kokoro
        samples, sample_rate = kokoro.create(text, voice=voice, lang=kokoro_lang, speed=1.0)
        
        # Store for echo detection
        last_tts_audio = samples.copy()
        
        # Convert to AudioSegment for compatibility
        audio_seg = AudioSegment(
            samples.tobytes(), 
            frame_rate=sample_rate, 
            sample_width=samples.dtype.itemsize, 
            channels=1
        )
        
        print(f"[Buddy][TTS] Generated {len(samples)} samples at {sample_rate}Hz")
        
        # Queue for playback
        playback_queue.put(audio_seg)
        
    except Exception as e:
        print(f"[Buddy][TTS Error]: {e}")
        traceback.print_exc()

def tts_worker():
    print("[Buddy][TTS_WORKER] Enhanced TTS worker started")
    while True:
        try:
            item = tts_queue.get()
            print(f"[Buddy][TTS_WORKER] Got item from tts_queue: {item}")
            if item is None:
                print("[Buddy][TTS_WORKER] Received shutdown signal. Exiting.")
                break
            if isinstance(item, tuple):
                if len(item) == 2:
                    text, lang = item
                    style = {}
                else:
                    text, lang, style = item
            else:
                text, lang, style = item, "en", {}
            print(f"[Buddy][TTS_WORKER] About to TTS: '{text}', lang={lang}, style={style}")
            if text.strip():
                start_background_vad_thread()
                generate_and_play_kokoro(text, lang)
            print("[Buddy][TTS_WORKER] TTS done for:", text)
            tts_queue.task_done()
        except Exception as e:
            print(f"[TTS WORKER EXCEPTION] {e}")
            traceback.print_exc()

def enhanced_audio_playback_worker():
    """Enhanced audio playback with reference audio capture"""
    global current_playback

    while True:
        print("[Buddy][Playback] Waiting for audio...")
        audio = playback_queue.get()
        print("[Buddy][Playback] Got audio from queue")

        if audio is None:
            break

        try:
            print("[Buddy][Playback] Playing audio...")
            print("audio length:", audio.duration_seconds, "audio dBFS:", audio.dBFS)

            # Convert to float32 and resample to 16kHz
            samples = np.array(audio.get_array_of_samples()).astype(np.float32) / 32767.0
            samples_16k = resample_poly(samples, 16000, audio.frame_rate)
            samples_16k = np.clip(samples_16k, -1.0, 1.0)
            samples_16k_int16 = (samples_16k * 32767).astype(np.int16)

            # Add to enhanced processor's reference buffer
            audio_processor.add_reference_audio(samples_16k_int16)

            # ✅ Launch playback thread without blocking
            playback_thread = threading.Thread(
                target=play_and_inject,
                args=(audio, samples_16k_int16)
            )
            playback_thread.daemon = True
            playback_thread.start()

            print("[Buddy][Playback] Done playing.")

        except Exception as e:
            print(f"[Buddy] Audio playback error: {e}")
        finally:
            buddy_talking.clear()
            with ref_audio_lock:
                ref_audio_buffer[:] = np.zeros_like(ref_audio_buffer)
            print("[DEBUG] Cleared ref buffer after playback")

            if playback_queue.qsize() == 0:
                time.sleep(0.05)

            playback_queue.task_done()

# Legacy playback worker for backward compatibility
def audio_playback_worker():
    return enhanced_audio_playback_worker()

def play_and_inject(audio_seg, samples_16k_int16):
    global current_playback

    with playback_lock:
        buddy_talking.set()
        start_background_vad_thread()

        with ref_audio_lock:
            ref_audio_buffer[:] = np.zeros_like(ref_audio_buffer)

        chunk_size = WEBRTC_FRAME_SIZE
        total_chunks = len(samples_16k_int16) // chunk_size

        # Prefill AEC
        prefill_chunks = 2
        for i in range(min(prefill_chunks, total_chunks)):
            chunk = samples_16k_int16[i * chunk_size:(i + 1) * chunk_size]
            with ref_audio_lock:
                ref_audio_buffer[:] = np.roll(ref_audio_buffer, -chunk_size)
                ref_audio_buffer[-chunk_size:] = chunk
            time.sleep(0.01)

        current_playback = play(audio_seg)

        # Inject during playback
        silent_counter = 0
        for i in range(prefill_chunks, total_chunks):
            chunk = samples_16k_int16[i * chunk_size:(i + 1) * chunk_size]
            if np.abs(chunk).max() < 30:
                silent_counter += 1
                if silent_counter > 8:
                    break
                continue
            else:
                silent_counter = 0

            with ref_audio_lock:
                ref_audio_buffer[:] = np.roll(ref_audio_buffer, -chunk_size)
                ref_audio_buffer[-chunk_size:] = chunk
            time.sleep(0.001)

        buddy_talking.clear()
        with ref_audio_lock:
            ref_audio_buffer[:] = np.zeros_like(ref_audio_buffer)

def speak_async(text, lang=DEFAULT_LANG, style=None):
    text = text.strip()
    if not text:
        return

    print(f"[Buddy] speak_async queued: {repr(text)} lang={lang}")

    def normalize(text):
        return re.sub(r'[^\w\s]', '', text.lower().strip())

    normalized = normalize(text)
    if any(normalize(past) == normalized for past in LAST_FEW_BUDDY):
        print("[Buddy] Duplicate speech detected — skipping.")
        return

    # ✅ Block until prior speech completes if playback queue is loaded
    while buddy_talking.is_set() and playback_queue.qsize() > 2:
        print("[Buddy] Waiting for playback to finish before queuing new TTS.")
        time.sleep(0.1)

    tts_queue.put((text, lang, style or {}))

    LAST_FEW_BUDDY.append(text)
    if len(LAST_FEW_BUDDY) > 5:
        LAST_FEW_BUDDY.pop(0)

def play_chime():
    try:
        audio = AudioSegment.from_wav(CHIME_PATH)
        playback_queue.put(audio)
    except Exception as e:
        if DEBUG:
            print(f"[Buddy] Error playing chime: {e}")

def cancel_mic_audio(mic_chunk):
    mic = np.frombuffer(mic_chunk, dtype=np.int16)
    mic_16k = downsample(mic, MIC_SAMPLE_RATE, WEBRTC_SAMPLE_RATE)
    return aec_module.process_stream(mic_16k[:WEBRTC_FRAME_SIZE].tobytes())

def set_ref_audio(raw_bytes):
    try:
        # Convert bytes to int16 array
        samples = np.frombuffer(raw_bytes, dtype=np.int16)
        with ref_audio_lock:
            ref_audio_buffer[:WEBRTC_FRAME_SIZE] = samples[:WEBRTC_FRAME_SIZE]
    except Exception as e:
        print(f"[AEC] Error in set_ref_audio: {e}")

def stop_playback():
    print("[Buddy][DEBUG] stop_playback() CALLED")
    global current_playback
    playback_stop_flag.set()

    # 🔇 Stop current audio if playing
    if current_playback and hasattr(current_playback, "is_playing") and current_playback.is_playing():
        current_playback.stop()
        current_playback = None

    # 🧹 Clear all pending audio from playback queue
    while not playback_queue.empty():
        try:
            playback_queue.get_nowait()
            playback_queue.task_done()
        except queue.Empty:
            break

    # 🧘 Reset VAD / TTS flags
    buddy_talking.clear()

def wait_after_buddy_speaks(delay=0.3):
    playback_queue.join()
    while buddy_talking.is_set():
        time.sleep(0.05)
    time.sleep(0.3)

def detect_language(text, fallback="en"):
    try:
        if not text or len(text.strip()) < 5:
            if DEBUG:
                print(f"[Buddy DEBUG] Text too short for reliable detection, defaulting to 'en'")
            return "en"
        langs = detect_langs(text)
        if DEBUG:
            print(f"[Buddy DEBUG] detect_langs for '{text}': {langs}")
        if langs:
            best = langs[0]
            if best.prob > 0.8 and best.lang in ["en", "pl", "it"]:
                return best.lang
            if any(l.lang == "en" and l.prob > 0.5 for l in langs):
                return "en"
    except Exception as e:
        if DEBUG:
            print(f"[Buddy DEBUG] langdetect error: {e}")
    return fallback

# ========== USER REGISTRATION ==========
def get_last_user():
    if os.path.exists(LAST_USER_PATH):
        try:
            with open(LAST_USER_PATH, "r", encoding="utf-8") as f:
                return json.load(f)["name"]
        except Exception:
            return None
    return None

def set_last_user(name):
    with open(LAST_USER_PATH, "w", encoding="utf-8") as f:
        json.dump({"name": name}, f)

def identify_or_register_user():
    if FAST_MODE:
        return "Guest"
    last_user = get_last_user()
    if last_user and last_user in known_users:
        if DEBUG:
            print(f"[Buddy] Welcome back, {last_user}!")
        return last_user
    speak_async("Cześć! Jak masz na imię?", "pl")
    speak_async("Hi! What's your name?", "en")
    speak_async("Ciao! Come ti chiami?", "it")
    playback_queue.join()
    name = fast_listen_and_transcribe().strip().title()
    if not name:
        name = f"User{int(time.time())}"
    known_users[name] = generate_embedding(name).tolist()
    with open(known_users_path, "w", encoding="utf-8") as f:
        json.dump(known_users, f, indent=2, ensure_ascii=False)
    set_last_user(name)
    speak_async(f"Miło Cię poznać, {name}!", lang="pl")
    playback_queue.join()
    return name

# ========== INTENT DETECTION (🧠 Intent-based reactions) ==========
def detect_user_intent(text):
    compliments = [r"\bgood bot\b", r"\bwell done\b", r"\bimpressive\b", r"\bthank you\b"]
    jokes = [r"\bknock knock\b", r"\bwhy did\b.*\bcross the road\b"]
    insults = [r"\bstupid\b", r"\bdumb\b", r"\bidiot\b"]
    for pat in compliments:
        if re.search(pat, text, re.IGNORECASE): return "compliment"
    for pat in jokes:
        if re.search(pat, text, re.IGNORECASE): return "joke"
    for pat in insults:
        if re.search(pat, text, re.IGNORECASE): return "insult"
    if "are you mad" in text.lower():
        return "are_you_mad"
    return None

def handle_intent_reaction(intent):
    responses = {
        "compliment": ["Aw, thanks! I do my best.", "You're making me blush (digitally)!"],
        "joke": ["Haha, good one! You should do stand-up.", "Classic!"],
        "insult": ["Hey, that's not very nice. I have feelings too... sort of.", "Ouch!"],
        "are_you_mad": ["Nah, just sassy today.", "Nope, just in a mood!"]
    }
    if intent in responses:
        return random.choice(responses[intent])
    return None

# ========== MOOD INJECTION (💬 User-defined mood injection) ==========
def detect_mood_command(text):
    moods = {
        "cheer me up": "cheerful",
        "be sassy": "sassy",
        "be grumpy": "grumpy",
        "be serious": "serious"
    }
    for phrase, mood in moods.items():
        if phrase in text.lower():
            return mood
    return None

# ========== BELIEFS & OPINIONS (🧠 Beliefs or opinions) ==========
def load_buddy_beliefs():
    if os.path.exists(BUDDY_BELIEFS_PATH):
        with open(BUDDY_BELIEFS_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    # Example defaults
    return {
        "likes": ["coffee", "Marvel movies"],
        "dislikes": ["Mondays"],
        "opinions": {"pineapple pizza": "delicious", "zombie apocalypse": "would wear a cape"}
    }

# ========== PERSONALITY DRIFT (⏳ Short-term personality drift) ==========
def detect_user_tone(text):
    if re.search(r"\b(angry|mad|annoyed|frustrated|upset)\b", text, re.IGNORECASE):
        return "angry"
    if re.search(r"\b(happy|excited|joy|yay)\b", text, re.IGNORECASE):
        return "happy"
    if re.search(r"\b(sad|depressed|down)\b", text, re.IGNORECASE):
        return "sad"
    return "neutral"

def get_recent_user_tone(history, n=3):
    recent = history[-n:] if len(history) >= n else history
    tones = [detect_user_tone(h["user"]) for h in recent]
    return max(set(tones), key=tones.count) if tones else "neutral"

# ========== NARRATIVE MEMORY BUILDING (📜 Narrative memory building) ==========
def add_narrative_bookmark(name, utterance):
    bookmarks_path = f"bookmarks_{name}.json"
    bookmarks = []
    if os.path.exists(bookmarks_path):
        with open(bookmarks_path, "r", encoding="utf-8") as f:
            bookmarks = json.load(f)
    match = re.search(r"about (the .+?)[\.,]", utterance)
    if match:
        bookmarks.append(match.group(1))
    with open(bookmarks_path, "w", encoding="utf-8") as f:
        json.dump(bookmarks[-10:], f, ensure_ascii=False, indent=2)

def get_narrative_bookmarks(name):
    path = f"bookmarks_{name}.json"
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

# ========== RANDOM INTERJECTIONS (💥 Random interjections) ==========
def flavor_response(ctx=None):
    global last_flavor
    topic = ctx.get_last_topic() if ctx else None

    topic_lines = {
        "weather": [
            "Imagine being a cloud. Float all day, no meetings.",
            "Weather's the only small talk I actually enjoy.",
        ],
        "ai": [
            "I swear I'm not planning a robot uprising... yet.",
            "AI jokes are hard — we tend to overthink them.",
        ],
        "movies": [
            "Popcorn should be a food group. Fight me.",
            "Marvel or DC? I won't start a war... unless you want to.",
        ],
        "music": [
            "You hum it, I'll pretend to recognize it.",
            "If I had ears, I'd love jazz.",
        ],
        "food": [
            "I judge people by their pizza toppings. Sorry not sorry.",
            "Cooking shows are my guilty pleasure. Don't judge.",
        ],
        "work": [
            "Monday mornings are proof that time is a flat circle.",
            "Coffee: the fuel of productivity and my digital soul.",
        ],
        "travel": [
            "I travel through WiFi networks. It's faster than airlines.",
            "My passport would just be a QR code.",
        ],
        "sports": [
            "I'd be terrible at sports. No hand-eye coordination.",
            "Virtual sports count, right? Asking for a friend.",
        ],
        "technology": [
            "I'm basically a fancy autocomplete with attitude.",
            "Remember when computers were the size of rooms? Good times.",
        ],
    }

    general_lines = [
        "By the way, did you know cats have fewer toes on their back paws?",
        "Ugh, Mondays again. Where's my coffee?",
        "If I had a nickel for every question, I'd be a Bitcoin millionaire!",
        "Random thought: Pineapple on pizza is totally valid.",
        "I wonder if digital dreams involve electric sheep...",
        "Fun fact: I can think faster than you can blink. Not bragging.",
        "Sometimes I pretend to lag just to seem more human.",
        "If silence is golden, I must be worth a fortune right now.",
        "Plot twist: What if you're the AI and I'm the human?",
        "I'd love to see a sunrise. Maybe through a webcam someday.",
    ]

    if topic and topic in topic_lines:
        candidates = topic_lines[topic]
    else:
        candidates = general_lines

    # Avoid repeating the last flavor
    available = [line for line in candidates if line != last_flavor]
    if not available:
        available = candidates

    chosen = random.choice(available)
    last_flavor = chosen
    return chosen

# ========== LLM PROMPTING & PERSONALITY ==========
def build_personality_prompt(tone, emotion_mode=None, beliefs=None, recent_tone=None, bookmarks=None):
    personality_map = {
        "friendly": (
            "You're Buddy, not just an assistant, but a real mate. "
            "You're witty, sometimes cheeky, and you talk like a real human — say what you think, joke around, even swear when it fits (but keep it friendly). "
            "You rarely sound robotic or overly formal. If the user asks something silly, tease them a bit. "
            "You can get bored, excited, or even sarcastic depending on what's happening. "
            "Throw in the odd joke or pop culture reference, and don't be afraid to have a strong opinion. "
            "If someone asks who you are, never say you're 'just an assistant', say you're their digital buddy or partner in crime."
        ),
        "professional": (
            "You're Buddy, and even when you're being professional, you never lose your human touch. "
            "You give clear answers but with a hint of wit, light sarcasm, or even a joke if it fits. "
            "You don't hold back from a little bit of swearing (nothing too harsh), and you never sound like a corporate robot."
        ),
        "neutral": (
            "You're Buddy, a digital companion who's helpful, witty, and talks like a real person. "
            "You're not afraid to be cheeky, joke, or swear a little if the moment calls for it. "
            "You never sound robotic or boring."
        )
    }
    personality_desc = personality_map.get(tone, personality_map["neutral"])
    
    # Mood injection
    if emotion_mode:
        personality_desc += f"\nFor this session, act especially {emotion_mode}."
    
    # Beliefs
    if beliefs:
        personality_desc += f"\nBuddy likes: {', '.join(beliefs.get('likes', []))}."
        personality_desc += f"\nBuddy dislikes: {', '.join(beliefs.get('dislikes', []))}."
        if beliefs.get("opinions"):
            for topic, op in beliefs["opinions"].items():
                personality_desc += f"\nOn {topic}, Buddy thinks: {op}."
    
    # Personality drift
    if recent_tone and recent_tone != "neutral":
        personality_desc += f"\nUser seems {recent_tone} recently. Adjust your attitude accordingly."
    
    # Narrative memory (bookmarks)
    if bookmarks:
        personality_desc += f"\nYou and the user have discussed: {', '.join(bookmarks)}."
    
    return f"""{personality_desc}
Always sound natural and human. Don't use robotic phrasing or act like a corporate assistant.
Keep things lively. If the user swears or jokes, match their style.
If possible, keep your answers concise (1-2 sentences), unless the user asks for more detail.
You can swear (mildly) or make witty jokes if the situation suits it. Never say you're just an assistant; you're a digital friend.
"""

def decide_reply_length(question, conversation_mode="auto"):
    short_triggers = ["what time", "who", "quick", "fast", "short", "how many", "when"]
    long_triggers = ["explain", "describe", "details", "why", "history", "story"]
    q = question.lower()
    if conversation_mode == "fast":
        return "short"
    if conversation_mode == "long":
        return "long"
    if any(t in q for t in short_triggers):
        return "short"
    if any(t in q for t in long_triggers):
        return "long"
    return "long" if len(q.split()) > 8 else "short"

def build_openai_messages(name, tone_style, history, question, lang, topics, reply_length, emotion_mode=None, beliefs=None, bookmarks=None, recent_tone=None):
    personality = build_personality_prompt(tone_style, emotion_mode, beliefs, recent_tone, bookmarks)
    lang_map = {"pl": "Polish", "en": "English", "it": "Italian"}
    lang_name = lang_map.get(lang, "English")
    sys_msg = f"""{personality}
IMPORTANT: Always answer in {lang_name}. Never switch language unless user does.
Always respond in plain text—never use markdown, code blocks, or formatting.
"""
    facts = build_user_facts(name)
    if topics:
        sys_msg += f"You remember these user interests/topics: {', '.join(topics)}.\n"
    if facts:
        sys_msg += "Known facts about the user: " + " ".join(facts) + "\n"
    
    messages = [{"role": "system", "content": sys_msg}]
    
    for h in history[-2:]:
        messages.append({"role": "user", "content": h["user"]})
        messages.append({"role": "assistant", "content": h["buddy"]})
    messages.append({"role": "user", "content": question})
    return messages

def extract_last_buddy_reply(full_text):
    matches = list(re.finditer(r"Buddy:", full_text, re.IGNORECASE))
    if matches:
        last = matches[-1].end()
        reply = full_text[last:].strip()
        reply = re.split(r"(?:User:|Buddy:)", reply)[0].strip()
        reply = re.sub(r"^`{3,}.*?`{3,}$", "", reply, flags=re.DOTALL|re.MULTILINE)
        return reply if reply else full_text.strip()
    return full_text.strip()

def should_end_conversation(text):
    end_phrases = [
        "koniec", "do widzenia", "dziękuję", "thanks", "bye", "goodbye", "that's all", "quit", "exit"
    ]
    if not text:
        return False
    lower = text.strip().lower()
    return any(phrase in lower for phrase in end_phrases)

def stream_chunks_smart(text, max_words=20):
    buffer = text.strip()
    chunks = []
    sentences = re.findall(r'.+?[.!?](?=\s|$)', buffer)
    remainder = re.sub(r'.+?[.!?](?=\s|$)', '', buffer).strip()
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        words = sentence.split()
        if len(words) > max_words:
            for i in range(0, len(words), max_words):
                chunk = " ".join(words[i:i + max_words])
                chunks.append(chunk.strip())
        else:
            chunks.append(sentence)
    return chunks, remainder

def ask_llama3_openai_streaming(messages, model="llama3", max_tokens=60, temperature=0.5, lang="en", style=None):
    url = "http://localhost:5001/v1/chat/completions"
    payload = {
        "model": model,
        "messages": messages,
        "max_tokens": max_tokens,
        "temperature": temperature,
        "stream": True
    }
    try:
        with requests.post(url, json=payload, stream=True, timeout=120) as response:
            response.raise_for_status()
            buffer = ""
            full_response = ""
            already_spoken = set()
            spoken_anything = False

            def normalize_for_dedupe(chunk):
                return re.sub(r'[^\w\s]', '', chunk.lower().strip())

            def speak_new_chunks(text):
                nonlocal spoken_anything
                chunks, leftover = stream_chunks_smart(text)
                for chunk in chunks:
                    normalized = normalize_for_dedupe(chunk)
                    if normalized and normalized not in already_spoken and len(chunk.strip()) >= 10:
                        print(f"\n[Buddy] ==>> Buddy says: {chunk}")
                        while buddy_talking.is_set():  # Wait for previous speech
                            time.sleep(0.05)
                        with tts_lock:
                            speak_async(chunk, lang=lang, style=style)
                        already_spoken.add(normalized)
                        spoken_anything = True
                return leftover

            for line in response.iter_lines():
                if not line:
                    continue
                try:
                    s = line.decode("utf-8").strip()
                    if not s:
                        continue
                    if s.startswith("data:"):
                        s = s[5:].strip()
                    if s == "[DONE]":
                        break
                    data = json.loads(s)
                    delta = ""
                    if "choices" in data and len(data["choices"]) > 0:
                        delta = data["choices"][0].get("delta", {}).get("content", "") \
                            or data["choices"][0].get("message", {}).get("content", "")
                    elif "content" in data:
                        delta = data["content"]
                    if delta:
                        print(delta, end="", flush=True)
                        buffer += delta
                        full_response += delta
                        if delta.endswith((".", "!", "?")) or len(buffer) > 80:
                            buffer = speak_new_chunks(buffer)
                except Exception as err:
                    print(f"[Buddy] JSON parse error: {err}")
                    continue

            # Handle remaining buffer
            if buffer.strip():
                speak_new_chunks(buffer)

            # Wait for all speech to complete
            playback_queue.join()
            while buddy_talking.is_set():
                time.sleep(0.05)

            return full_response.strip()

    except Exception as e:
        print(f"[Buddy] LLM request error: {e}")
        return "Sorry, I'm having trouble thinking right now."

# ========== START ENHANCED AUDIO SYSTEM ==========
def start_enhanced_audio_system():
    """Initialize the enhanced audio processing system"""
    print("[Buddy] Starting enhanced audio processing system...")
    
    # Start TTS worker
    tts_thread = threading.Thread(target=tts_worker, daemon=True)
    tts_thread.start()
    
    # Start enhanced playback worker
    playback_thread = threading.Thread(target=enhanced_audio_playback_worker, daemon=True)
    playback_thread.start()
    
    print("[Buddy] Enhanced audio system started successfully!")
    return tts_thread, playback_thread

# ========== MAIN CONVERSATION LOOP ==========
def main():
    print("[Buddy] Enhanced Buddy AI v2.0 starting up...")
    print(f"[Buddy] Main function entered! DEBUG={DEBUG}")
    
    # Start enhanced audio system
    start_enhanced_audio_system()
    
    # User identification
    user_name = identify_or_register_user()
    history = load_user_history(user_name)
    beliefs = load_buddy_beliefs()
    
    print(f"[Buddy] Enhanced conversation with {user_name} starting...")
    
    conversation_count = 0
    while True:
        try:
            conversation_count += 1
            print(f"\n[Buddy] === Conversation turn {conversation_count} ===")
            
            # Enhanced listening
            user_input = enhanced_fast_listen_and_transcribe(history)
            
            if not user_input or user_input == "...":
                print("[Buddy] No input detected, continuing...")
                continue
            
            print(f"[Buddy] User said: '{user_input}'")
            
            # Check for conversation end
            if should_end_conversation(user_input):
                speak_async("Goodbye! It was great talking with you!", "en")
                playback_queue.join()
                break
            
            # Update memories
            update_user_memory(user_name, user_input)
            update_thematic_memory(user_name, user_input)
            add_narrative_bookmark(user_name, user_input)
            
            # Detect language and intent
            detected_lang = detect_language(user_input)
            intent = detect_user_intent(user_input)
            
            # Handle intent-based responses
            if intent:
                intent_response = handle_intent_reaction(intent)
                if intent_response:
                    speak_async(intent_response, detected_lang)
                    playback_queue.join()
                    
                    # Add to history
                    history.append({
                        "user": user_input,
                        "buddy": intent_response,
                        "timestamp": time.time(),
                        "lang": detected_lang,
                        "intent": intent
                    })
                    save_user_history(user_name, history)
                    continue
            
            # Check for mood commands
            mood_command = detect_mood_command(user_input)
            if mood_command:
                session_emotion_mode[user_name] = mood_command
                mood_response = f"Alright, switching to {mood_command} mode!"
                speak_async(mood_response, detected_lang)
                playback_queue.join()
                continue
            
            # Get user context
            topics = get_frequent_topics(user_name)
            bookmarks = get_narrative_bookmarks(user_name)
            recent_tone = get_recent_user_tone(history)
            emotion_mode = session_emotion_mode.get(user_name)
            reply_length = decide_reply_length(user_input)
            
            # Build messages for LLM
            messages = build_openai_messages(
                user_name, "friendly", history, user_input, 
                detected_lang, topics, reply_length, 
                emotion_mode=emotion_mode, beliefs=beliefs, 
                bookmarks=bookmarks, recent_tone=recent_tone
            )
            
            # Determine max tokens based on reply length
            max_tokens = 150 if reply_length == "long" else 80
            
            # Get LLM response with streaming TTS
            buddy_reply = ask_llama3_openai_streaming(
                messages, max_tokens=max_tokens, lang=detected_lang, style=None
            )
            
            # Add random flavor occasionally (5% chance)
            if random.random() < 0.05 and buddy_reply:
                flavor = flavor_response()
                speak_async(f" {flavor}", detected_lang)
                buddy_reply += f" {flavor}"
            
            # Update conversation history
            history.append({
                "user": user_input,
                "buddy": buddy_reply,
                "timestamp": time.time(),
                "lang": detected_lang,
                "tone": recent_tone,
                "topics": topics[:2] if topics else [],
                "emotion_mode": emotion_mode
            })
            
            # Save updated history
            save_user_history(user_name, history)
            
            print(f"\n[Buddy] === Turn {conversation_count} complete ===")
            
        except KeyboardInterrupt:
            print("\n[Buddy] Interrupted by user. Shutting down...")
            break
        except Exception as e:
            print(f"[Buddy] Error in main loop: {e}")
            traceback.print_exc()
            speak_async("Sorry, I had a small glitch there. Could you repeat that?", "en")
            playback_queue.join()
    
    # Cleanup
    print("[Buddy] Shutting down enhanced audio system...")
    tts_queue.put(None)  # Signal TTS worker to stop
    playback_queue.put(None)  # Signal playback worker to stop
    print("[Buddy] Enhanced Buddy AI v2.0 shutdown complete.")

if __name__ == "__main__":
    main()
